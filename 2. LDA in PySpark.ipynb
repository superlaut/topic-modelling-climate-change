{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA in PySpark\n",
    "\n",
    "In this notebook an LDA is carried out on all four periods. First, however, a pipeline is built to convert the raw text into the right format for LDA. First a Bag of Words approach is used, using a CountVectorizer. Then the speed of this approach is compared to that of using the output of the Bert Embeddings from 1. \n",
    "\n",
    "To run this on Google Cloud Platform, the followings instructions are to be followed.\n",
    "\n",
    "1. First, make a bucket. Upload spark-nlp-init.sh and json dataset into that bucket.\n",
    "2. Then, create a cluster: gcloud dataproc clusters create laurens-cluster --bucket=laurens-bucket --subnet default --zone europe-west2-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 1.3-deb9 --initialization-actions 'gs://dataproc-initialization-actions/jupyter/jupyter.sh,gs://dataproc-initialization-actions/python/pip-install.sh,gs://laurens-bucket/spark-nlp-init.sh' --metadata 'PIP_PACKAGES=sklearn nltk pandas numpy textblob spark-nlp'\n",
    "3. Then run \"gcloud beta compute ssh --zone \"europe-west2-a\" \"laurens-cluster-m\" --project \"seminar-work-st446\"\"\n",
    "4. Within the cluster, gsutil cp gs://laurens-bucket/* .\n",
    "4. Then do hadoop fs -mkdir txtdata, then hadoop fs -put climateChangeArticlesGuardian0019.json txtdata\n",
    "5. In tab #0, gcloud compute ssh laurens-cluster-m --project=seminar-work-st446 --zone=europe-west2-a -- -D 1080 -N\n",
    "6. In tab #1, /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --proxy-server=socks5://localhost:1080 --user-data-dir=/tmp/laurens-cluster-mÂ http://laurens-cluster-m:8123\n",
    "\n",
    "### 2.0 Importing packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # test\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import string\n",
    "\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bodyText: string (nullable = true)\n",
      " |-- webPublicationDate: timestamp (nullable = true)\n",
      " |-- wordcount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonfolder = \"hdfs:///user/superlaut/txtdata/climateChangeArticlesGuardian0019.json\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"bodyText\", StringType(), True),\n",
    "    StructField(\"webPublicationDate\", TimestampType(), True),\n",
    "    StructField(\"wordcount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.json(jsonfolder, schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into four periods\n",
    "df0004 = df.where(df.webPublicationDate.between('2000-01-01','2004-12-31'))\n",
    "df0509 = df.where(df.webPublicationDate.between('2005-01-01','2009-12-31'))\n",
    "df1014 = df.where(df.webPublicationDate.between('2010-01-01','2014-12-31'))\n",
    "df1519 = df.where(df.webPublicationDate.between('2015-01-01','2019-12-04'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  6\n",
      "Number of documents n =  21001\n"
     ]
    }
   ],
   "source": [
    "# This operation is carried out to transform the df into an RDD. \n",
    "news = df.select(\"bodyText\")\n",
    "news = news.toJSON()\n",
    "print('Number of partitions: ', news.getNumPartitions())\n",
    "n = news.count()\n",
    "print('Number of documents n = ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LDA\n",
    "\n",
    "First, we find the stopwords using the approach from the seminar. The doc_stop_words selects all words that appear more than 5,000 times in the raw text. From these, I manually select the words that I do not consider relevant for topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def get_tokens(line):\n",
    "    ###\n",
    "    import nltk\n",
    "#     nltk.download('all')\n",
    "    ###\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuations from each word\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatizing the words, see https://en.wikipedia.org/wiki/Lemmatisation\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    return (words)\n",
    "\n",
    "news_rdd = news.map(lambda line: (1, get_tokens(line)))\n",
    "doc_stop_words = news_rdd.flatMap(lambda r: r[1]).map(lambda r: (r,1)).reduceByKey(lambda a,b: a+b)\n",
    "doc_stop_words = doc_stop_words.filter(lambda a: a[1]>5000).map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stop_words = ['something', 'le', 'un', 'could', 'keep', 'thing', 'must', 'find', 'however', \n",
    "                  'one', 'really', 'put', 'given', 'whether', 'make', 'way', 'nt', 'told', 'come',\n",
    "                  'would', 'three', 'give', 'mr', 'like', 'also', 'u', 'rather', 'made', 'mean', \n",
    "                  'may', 'seen', 'bodytext', 'set', 'much', 'become', 'two', 'even', 'might', \n",
    "                  'around', 'going', 'per', 'many', 'said', 'say']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline LDA\n",
    "Here I create a pipeline to convert the raw text into cleaned tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove non ASCII characters. Not necessarily relevant, but the text might contain some ASCII characters.\n",
    "def strip_non_ascii(data_str):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "# Using user-defined functions, again, so that PySpark sql recognises the function.\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
    "\n",
    "# Removing features\n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 1 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 1:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 1:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    # remove unwanted space, *.split() will automatically split on\n",
    "    # whitespace and discard duplicates, the \" \".join() joins the\n",
    "    # resulting list into one string.\n",
    "    return \" \".join(cleaned_str.split())\n",
    "\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "\n",
    "# Changing abbreviations into full words. Probably not relevant, as these words\n",
    "# are probably not topic modelled.\n",
    "def fix_abbreviation(data_str):\n",
    "    data_str = data_str.lower()\n",
    "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
    "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
    "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
    "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
    "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
    "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
    "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
    "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
    "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
    "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
    "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
    "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
    "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
    "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
    "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
    "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
    "    data_str = re.sub(r'rt\\b', '', data_str)\n",
    "    data_str = data_str.strip()\n",
    "    return data_str\n",
    "\n",
    "fix_abbreviation_udf = udf(fix_abbreviation, StringType())\n",
    "\n",
    "# Removing stop words\n",
    "def remove_stops(data_str):\n",
    "    # expects a string\n",
    "    stops = list(set(stopwords.words(\"english\"))) + ['something', 'le', 'un', 'could', 'keep', 'thing', 'must', 'find', 'however', \n",
    "                  'one', 'really', 'put', 'given', 'whether', 'make', 'way', 'nt', 'told', 'come',\n",
    "                  'would', 'three', 'give', 'mr', 'like', 'also', 'u', 'rather', 'made', 'mean', \n",
    "                  'may', 'seen', 'bodytext', 'set', 'much', 'become', 'two', 'even', 'might', \n",
    "                  'around', 'going', 'per', 'many', 'said', 'say']\n",
    "    \n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stops:\n",
    "            # rebuild cleaned_str\n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "\n",
    "# Lemmatizing, using WordNetLemmatizer\n",
    "def lemmatize(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = data_str.split()\n",
    "    tagged_words = pos_tag(text)\n",
    "    for word in tagged_words:\n",
    "        if 'v' in word[1].lower():\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
    "        else:\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
    "        if list_pos == 0:\n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "            cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "lemmatize_udf = udf(lemmatize, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, vectorizer, and lda.\n",
    "tokenizer = Tokenizer(inputCol=\"lemm_text\", outputCol=\"tokens\")\n",
    "vectorizer = CountVectorizer(inputCol= \"tokens\", outputCol=\"features\")\n",
    "\n",
    "# I have finetuned the LDA parameters, leading to a low perplexity for df0004.\n",
    "# By assumption, this works for the other dataframes too.\n",
    "lda = LDA(k=10, maxIter=30, optimizer = 'em', learningOffset=1024.0, \n",
    "          learningDecay=0.4, subsamplingRate=0.05, \n",
    "          optimizeDocConcentration=True, seed = 1)\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, lda])\n",
    "\n",
    "# I could have included the udf as part of the pipeline, but this would lead\n",
    "# to really ugly code, including a lot of SQLTransformers. Instead, \n",
    "# I wrote a function.\n",
    "def LDApipeline(df_0):\n",
    "    start = time.time()\n",
    "    df_0 = df_0.withColumn('text_non_asci',strip_non_ascii_udf(df_0['bodyText']))\n",
    "    df_0 = df_0.withColumn('removed',remove_features_udf(df_0['text_non_asci']))\n",
    "    df_0 = df_0.withColumn('non_abbrev',fix_abbreviation_udf(df_0['removed']))\n",
    "    df_0 = df_0.withColumn('stop_text',remove_stops_udf(df_0['non_abbrev']))\n",
    "    df_0 = df_0.withColumn('lemm_text',lemmatize_udf(df_0['stop_text']))\n",
    "    \n",
    "    model = pipeline.fit(df_0)\n",
    "    end = time.time()\n",
    "    dt = end - start\n",
    "    print('Operation cost',dt,'seconds')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation cost 126.04530048370361 seconds\n"
     ]
    }
   ],
   "source": [
    "model0004 = LDApipeline(df0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation cost 434.1915535926819 seconds\n"
     ]
    }
   ],
   "source": [
    "model0509 = LDApipeline(df0509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation cost 407.7295799255371 seconds\n"
     ]
    }
   ],
   "source": [
    "model1014 = LDApipeline(df1014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation cost 616.1662304401398 seconds\n"
     ]
    }
   ],
   "source": [
    "model1519 = LDApipeline(df1519)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the data and creating the LDA models takes very long. This is understandable, as the dataframes have huge vocabulaires (> 26k, 67k even for the period from 2015-2019). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 00-04 = 26148\n",
      "Vocab size 05-09 = 44922\n",
      "Vocab size 00-04 = 46250\n",
      "Vocab size 00-04 = 67406\n"
     ]
    }
   ],
   "source": [
    "lda_0004 = model0004.stages[-1]\n",
    "print('Vocab size 00-04 =',lda_0004.vocabSize())\n",
    "lda_0509 = model0509.stages[-1]\n",
    "print('Vocab size 05-09 =',lda_0509.vocabSize())\n",
    "lda_1014 = model1014.stages[-1]\n",
    "print('Vocab size 00-04 =',lda_1014.vocabSize())\n",
    "lda_1519 = model1519.stages[-1]\n",
    "print('Vocab size 00-04 =',lda_1519.vocabSize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below describes the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "['people' 'water' 'island' 'town' 'home' 'kill' 'mile']\n",
      "['year' 'climate' 'change' 'warm' 'global' 'scientist' 'ice']\n",
      "['people' 'world' 'government' 'africa' 'country' 'aid' 'food']\n",
      "['climate' 'country' 'world' 'change' 'emission' 'global' 'kyoto']\n",
      "['day' 'get' 'time' 'people' 'work' 'say' 'take']\n",
      "['weather' 'rain' 'flood' 'yesterday' 'south' 'north' 'east']\n",
      "['energy' 'government' 'environment' 'power' 'development' 'summit'\n",
      " 'minister']\n",
      "['u' 'bush' 'world' 'global' 'president' 'state' 'american']\n",
      "['flood' 'water' 'year' 'area' 'river' 'risk' 'home']\n",
      "['earthquake' 'people' 'city' 'rescue' 'building' 'quake' 'kill']\n"
     ]
    }
   ],
   "source": [
    "# Describe topics\n",
    "topics0004 = lda_0004.describeTopics(7)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "# Shows the results\n",
    "topic_i = topics0004.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(model0004.stages[-2].vocabulary)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "['change' 'climate' 'government' 'need' 'political' 'take' 'issue']\n",
      "['use' 'energy' 'think' 'people' 'get' 'say' 'home']\n",
      "['power' 'car' 'government' 'plan' 'new' 'green' 'station']\n",
      "['energy' 'oil' 'climate' 'change' 'world' 'carbon' 'government']\n",
      "['people' 'police' 'city' 'flood' 'area' 'water' 'day']\n",
      "['year' 'water' 'food' 'tree' 'ice' 'world' 'land']\n",
      "['climate' 'country' 'emission' 'change' 'u' 'global' 'target']\n",
      "['year' 'people' 'say' 'change' 'day' 'time' 'climate']\n",
      "['carbon' 'emission' 'government' 'energy' 'uk' 'reduce' 'year']\n",
      "['climate' 'change' 'global' 'scientist' 'warm' 'rise' 'temperature']\n"
     ]
    }
   ],
   "source": [
    "# Describe topics\n",
    "topics0509 = lda_0509.describeTopics(7)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "# Shows the results\n",
    "topic_i = topics0509.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(model0509.stages[-2].vocabulary)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "['government' 'green' 'environment' 'change' 'plan' 'climate' 'uk']\n",
      "['country' 'emission' 'climate' 'develop' 'china' 'world' 'u']\n",
      "['climate' 'government' 'policy' 'carbon' 'change' 'australia' 'energy']\n",
      "['energy' 'carbon' 'gas' 'uk' 'power' 'emission' 'use']\n",
      "['climate' 'change' 'water' 'flood' 'year' 'obama' 'people']\n",
      "['climate' 'warm' 'change' 'temperature' 'year' 'global' 'ice']\n",
      "['world' 'change' 'oil' 'climate' 'fossil' 'fuel' 'year']\n",
      "['climate' 'change' 'repo' 'global' 'world' 'new' 'u']\n",
      "['get' 'think' 'know' 'time' 'people' 'go' 'year']\n",
      "['climate' 'science' 'change' 'scientist' 'people' 'scientific' 'public']\n"
     ]
    }
   ],
   "source": [
    "# Describe topics\n",
    "topics1014 = lda_1014.describeTopics(7)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "# Shows the results\n",
    "topic_i = topics1014.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(model1014.stages[-2].vocabulary)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "['climate' 'change' 'government' 'country' 'australia' 'people' 'say']\n",
      "['emission' 'carbon' 'energy' 'country' 'climate' 'paris' 'gas']\n",
      "['climate' 'people' 'take' 'change' 'action' 'crisis' 'u']\n",
      "['climate' 'change' 'u' 'people' 'trump' 'global' 'year']\n",
      "['fire' 'year' 'record' 'city' 'temperature' 'water' 'weather']\n",
      "['climate' 'change' 'fossil' 'global' 'company' 'world' 'fuel']\n",
      "['year' 'ice' 'change' 'warm' 'temperature' 'sea' 'reef']\n",
      "['year' 'water' 'food' 'tree' 'plant' 'people' 'say']\n",
      "['climate' 'university' 'change' 'science' 'london' 'guardian' 'letter']\n",
      "['government' 'energy' 'policy' 'coal' 'australia' 'climate' 'minister']\n"
     ]
    }
   ],
   "source": [
    "# Describe topics\n",
    "topics1519 = lda_1519.describeTopics(7)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "# Shows the results\n",
    "topic_i = topics1519.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(model1519.stages[-2].vocabulary)[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Embeddings versus Bag of Words\n",
    "The code below imports the Bert Embeddings from the Spark NLP file 1. An LDA model of the same type is made for both output from the df0004 dataframe, after transforming its output into a format readable by LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- featureslist: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonfolder2 = \"hdfs:///user/superlaut/guardian_features.json\"\n",
    "\n",
    "df_vec = spark.read.json(jsonfolder2)\n",
    "df_vec = df_vec.withColumnRenamed('features', 'featureslist')\n",
    "df_vec.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector, Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# The vector has to be in VectorUDT format for PySpark, thus I convert it here.\n",
    "to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())\n",
    "df_vec_feat = df_vec.withColumn(\"features\", to_vector(df_vec[\"featureslist\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation cost 34.5931761264801 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lda0004 = lda.fit(df_vec_feat)\n",
    "end = time.time()\n",
    "dt = end - start\n",
    "print('Operation cost',dt,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda0004.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entire operation from the pipeline, without fitting the LDA, so that the LDA operation can be fairly timed.\n",
    "tokenizer = Tokenizer(inputCol=\"lemm_text\", outputCol=\"tokens\")\n",
    "vectorizer = CountVectorizer(inputCol= \"tokens\", outputCol=\"features\")\n",
    "lda = LDA(k=10, maxIter=30, optimizer = 'em', learningOffset=1024.0, \n",
    "          learningDecay=0.4, subsamplingRate=0.05, \n",
    "          optimizeDocConcentration=True, seed = 1)\n",
    "\n",
    "df_0 = df0004\n",
    "df_0 = df_0.withColumn('text_non_asci',strip_non_ascii_udf(df_0['bodyText']))\n",
    "df_0 = df_0.withColumn('removed',remove_features_udf(df_0['text_non_asci']))\n",
    "df_0 = df_0.withColumn('non_abbrev',fix_abbreviation_udf(df_0['removed']))\n",
    "df_0 = df_0.withColumn('stop_text',remove_stops_udf(df_0['non_abbrev']))\n",
    "df_0 = df_0.withColumn('lemm_text',lemmatize_udf(df_0['stop_text']))\n",
    "tokenised = tokenizer.transform(df_0)\n",
    "cv_model = vectorizer.fit(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_2 = cv_model.transform(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation cost 105.92472839355469 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lda_old = lda.fit(vectorized_2)\n",
    "end = time.time()\n",
    "dt = end - start\n",
    "print('Operation cost',dt,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the LDA on the embeddings is much faster. In fact, it is 3x faster. This is understandable as, instead of a large vector containing a count for all different words within each document (more than 26,000 in total), the LDA model is fed a vector of size 768 for each document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
