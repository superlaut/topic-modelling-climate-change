{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Using the Guardian API to gather all articles on Climate Change\n",
    "The code-block below creates a folder in which for each day all articles with the tag 'environment/climate-change' are gathered in a separate json-file. An API-key has been obtained by filling out a developer application for the Guardian API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Make directory\n",
    "ARTICLES_DIR = join('guardian', 'articles')\n",
    "makedirs(ARTICLES_DIR, exist_ok=True)\n",
    "\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "my_params = {\n",
    "    'tag' : \"environment/climate-change\",\n",
    "    'from-date': \"\",\n",
    "    'to-date': \"\",\n",
    "    'order-by': \"newest\",\n",
    "    'show-fields': 'all',\n",
    "    'page-size': 200,\n",
    "    'api-key': '7ca76969-9b4f-4186-a7f4-0965214465b7'\n",
    "}\n",
    "\n",
    "# Select dates for which I want articles\n",
    "start_date = date(2000, 01, 01)\n",
    "end_date = date(2019,12, 05)\n",
    "dayrange = range((end_date - start_date).days + 1)\n",
    "\n",
    "# Gather json files for each day\n",
    "for daycount in dayrange:\n",
    "    dt = start_date + timedelta(days=daycount)\n",
    "    datestr = dt.strftime('%Y-%m-%d')\n",
    "    filename = join(ARTICLES_DIR, datestr + '.json')\n",
    "    if not exists(filename):\n",
    "        all_results = []\n",
    "        my_params['from-date'] = datestr\n",
    "        my_params['to-date'] = datestr\n",
    "        current_page = 1\n",
    "        total_pages = 1\n",
    "        while current_page <= total_pages:\n",
    "            my_params['page'] = current_page\n",
    "            resp = requests.get(API_ENDPOINT, my_params)\n",
    "            data = resp.json()\n",
    "            all_results.extend(data['response']['results'])\n",
    "            current_page += 1\n",
    "            total_pages = data['response']['pages']\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            #print(\"Writing to\", filename)\n",
    "\n",
    "            # re-serialize it for pretty indentation\n",
    "            f.write(json.dumps(all_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below turns all different json files into one json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "\n",
    "dates = pd.date_range(start = '2000-01-01', end = '2019-12-04')\n",
    "\n",
    "allArticles = pd.DataFrame()\n",
    "for date in dates:\n",
    "    #print(date)\n",
    "    datestring = str(date).split(' ')[0]\n",
    "    articleInDay = pd.read_json('../project-2020-superlaut/guardian/articles/' + datestring + '.json')\n",
    "    allArticles = pd.concat([allArticles, articleInDay], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all articles are now gathered in one pandas DataFrame, now the relevant columns are selected. Selecting just three column and turning it into one file instead of multiple makes the file 100 MB in size, instead of 300 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "guardian = allArticles\n",
    "guardian[['standfirst', 'bodyText', 'wordcount']] = pd.io.json.json_normalize(guardian['fields'])[['standfirst', 'bodyText', 'wordcount']]\n",
    "guardian = guardian[['id', 'sectionName','webPublicationDate','webTitle', 'standfirst', 'bodyText', 'wordcount']]\n",
    "guardian['wordcount'] = guardian['wordcount'].astype(dtype = 'int64')\n",
    "guardian = guardian[['bodyText','webPublicationDate','wordcount']]\n",
    "# orient 'records' because PySpark cannot read any other format, this took me some time to figure out...\n",
    "guardian.to_json('../project-2020-superlaut/climateChangeArticlesGuardian0019.json', orient = 'records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
